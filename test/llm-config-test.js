#!/usr/bin/env node

/**
 * LLMé…ç½®æµ‹è¯• - éªŒè¯é…ç½®æ˜¯å¦æ­£ç¡®è¯»å–å’Œä½¿ç”¨
 */

import LLMService from '../core/services/llm.js';

async function testLLMConfiguration() {
    console.log('ğŸ” LLMé…ç½®æµ‹è¯•\n');

    // ä½¿ç”¨ä¸æœåŠ¡å™¨ç›¸åŒçš„é…ç½®ç»“æ„
    const config = {
        services: {
            llm: {
                provider: 'glm',
                model: 'glm-4-flash',
                api_key: '60284c17c64043f290fab4b0ce20ec1c.2ocJCaVIXzpGbch3',
                base_url: 'https://open.bigmodel.cn/api/paas/v4',
                temperature: 0.7,
                max_tokens: 500
            }
        }
    };

    try {
        console.log('1. åˆ›å»ºLLMæœåŠ¡å®ä¾‹...');
        const llmService = new LLMService(config);

        console.log('2. æ£€æŸ¥é…ç½®è¯»å–...');
        console.log('   Provider:', llmService.provider);
        console.log('   Model:', llmService.model);
        console.log('   API Keyé…ç½®:', llmService.apiKey ? 'âœ“ å·²é…ç½®' : 'âœ— æœªé…ç½®');
        console.log('   Base URL:', llmService.baseUrl);

        console.log('\n3. æ£€æŸ¥æœåŠ¡é…ç½®çŠ¶æ€...');
        console.log('   isConfigured():', llmService.isConfigured());

        console.log('\n4. æµ‹è¯•LLMè°ƒç”¨...');
        const response = await llmService.chat('test_connection', 'ä½ å¥½');
        console.log('   âœ“ LLMè°ƒç”¨æˆåŠŸ!');
        console.log('   å›å¤å†…å®¹:', response.substring(0, 100) + '...');

    } catch (error) {
        console.log('   âŒ LLMæµ‹è¯•å¤±è´¥:', error.message);
        console.log('   é”™è¯¯è¯¦æƒ…:', error);
    }
}

testLLMConfiguration().catch(console.error);